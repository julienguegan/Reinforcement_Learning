{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=\"6\"><center>Concepts clé</center></font>**\n",
    "\n",
    "<center> from <a href=\"https://spinningup.openai.com/en/latest/\">spinningup.openai.com</a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le problème de RL\n",
    "\n",
    "**Le but est de trouver la politique qui maximise la récompense attendue de l'agent.**\n",
    "\n",
    "Pour des transitions d'environnement $P$ et une politique $\\pi$ stochastique, la probabilité d'une trajectoire $\\tau$ (*séquence d'états et d'actions*) est :\n",
    "\n",
    "$$ P(\\tau|\\pi) = \\rho_0(s_0) \\prod_{t=0}^{T-1} P(s_{t+1}|s_t,a_t)\\pi(a_t|s_t) $$\n",
    "\n",
    "la récompense attendue est alors :\n",
    "\n",
    "$$ J(\\pi) = \\int_\\tau P(\\tau|\\pi)R(\\tau) = E_{\\tau\\sim\\pi}[R(\\tau)]  $$\n",
    "\n",
    "On peut alors exprimer le problème d'optimisation par :\n",
    "\n",
    "$$ \\pi^* = \\arg \\underset{\\pi}{\\max} J(\\pi) $$ où $\\pi^*$ est la **politique optimale**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les fonctions valeur\n",
    "\n",
    "Il est utile de connaître la valeur d'une paire état-action, c'est-à-dire la récompense attendue si on commence dans un état-action fixé puis agissons selon une politique particulière pour toujours.\n",
    "\n",
    "1. **fonction valeur sur politique** $V^\\pi(s)$ : renvoie la récompense attendue si on commence dans l'état $s$ et qu'on agit toujours selon la politique $\\pi$\n",
    "\n",
    "$$ V^\\pi(s) =  \\underset{\\tau\\sim\\pi}{E}[R(\\tau)|s_0=s] $$\n",
    "\n",
    "2.  **fonction action-valeur sur politique** $Q^\\pi(s,a)$ : renvoie la récompense attendue si on commence dans l'état $s$, on prend un action $a$ arbitraire et qu'on agit toujours selon la politique $\\pi$\n",
    "\n",
    "$$ Q^\\pi(s,a) =  E_{\\tau\\sim\\pi}[R(\\tau)|s_0=s,a_0=a] $$\n",
    "\n",
    "3. **fonction valeur optimal** $V^*(s)$ : renvoie la récompense attendue si on commence dans l'état $s$ et qu'on agit toujours selon la *politique optimal* dans l'environnement\n",
    "\n",
    "$$ V^*(s) = \\underset{\\pi}{\\max}\\underset{\\tau\\sim\\pi}{E}[R(\\tau)|s_0=s] $$\n",
    "\n",
    "4. **fonction valeur optimal** $Q^*(s,a)$ : renvoie la récompense attendue si on commence dans l'état $s$, on prend une action $a$ arbitraire et qu'on agit toujours selon la *politique optimal* dans l'environnement\n",
    "\n",
    "$$ Q^*(s,a) = \\underset{\\pi}{\\max}\\underset{\\tau\\sim\\pi}{E}[R(\\tau)|s_0=s,a_0=a]$$    \n",
    "  \n",
    "<br/>        \n",
    "        \n",
    "<u>note</u> : *Les fonctions valeur et action-valeur sont reliées par : $ V^\\pi(s) = \\underset{a\\sim\\pi}{E}[Q^\\pi(s,a)] $*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equations de Bellman\n",
    "\n",
    "<u>idée</u> : La valeur du point de départ est la récompense qu'on attendrait avoir d'être à ce point plus la valeur de l'endroit où on atterrirait ensuite.\n",
    "\n",
    "\n",
    "**fonction valeur sur politique** \n",
    "\n",
    "$$ V^\\pi(s) =  \\underset{\\underset{s'\\sim P}{a\\sim\\pi}}{E}[r(s,a)+\\gamma V^\\pi(s')] $$\n",
    "\n",
    "$$ q^\\pi(s,a) =  \\underset{s'\\sim P}{E}[r(s,a)+\\gamma \\underset{a'\\sim\\pi}{E}[Q^\\pi(s',a')]  $$\n",
    "\n",
    "**fonction valeur optimale** \n",
    "\n",
    "$$ V^\\pi(s) = \\underset{a}{\\max} \\underset{s'\\sim P}{E}[r(s,a)+\\gamma V^\\pi(s')] $$\n",
    "\n",
    "$$ q^\\pi(s,a) = \\underset{s'\\sim P}{E}[r(s,a)+\\gamma \\underset{a'}{\\max}E[Q^\\pi(s',a')]  $$\n",
    "\n",
    "<br/>       \n",
    "\n",
    "*La différence cruciale entre les équations de Bellman pour les fonctions de valeur sur la politique et les fonctions de valeur optimales est l'absence ou la présence de $\\max$ sur les actions. Son inclusion reflète le fait que chaque fois que l'agent arrive à choisir son action, pour agir de manière optimale, il doit choisir celle qui mène à la valeur la plus élevée.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour atteindre la fonction optimale de politique et de valeur, un procédé généralement utilisé est **l'itération de politique** qui consiste à alterner entre évaluation de la politique et amélioration de politique en s'appuyant sur la fonction valeur. Une fois qu'il y a convergence, c'est-à-dire que la politique n'est plus mise à jour, on obtient la politique optimale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmes RL\n",
    "\n",
    "<img src=\"https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec Modèle\n",
    "\n",
    "Le principal avantage d'avoir un modèle est qu'il permet à l'agent de **planifier en anticipant**, en voyant ce qui se passerait pour une gamme de choix possibles et en décidant explicitement entre ses options. Les agents peuvent ensuite distribuer les résultats de la planification à l'avance dans une politique apprise. \n",
    "\n",
    "Cependant, *un modèle de l'environnement n'est généralement pas disponible pour l'agent.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sans Modèle\n",
    "\n",
    "L'agent doit **apprendre le modèle par expérience**, ce qui crée plusieurs défis. Le plus grand défi est que le biais dans le modèle peut être exploité par l'agent, résultant en un agent qui fonctionne bien par rapport au modèle appris, mais se comporte de manière sous-optimale (ou terriblement) dans l'environnement réel. L'apprentissage du modèle est souvent difficile : un effort intense - être prêt à consacrer beaucoup de temps à le calculer - peut ne pas porter ses fruits.\n",
    "\n",
    "\n",
    "* **Optimisation de politique**\n",
    "\n",
    "Les méthodes de cette famille représente explicitement une politique $\\pi_\\theta(a|s)$. Elles optimisent les paramètres $\\theta$ par montée de gradient de la fonction objectif $J(\\pi_\\theta)$ (ou une approximation). Cette optimisation est réalisée **sur-politique**, chaque mise à jour utilise uniquement les données collectées tout en agissant selon la version la plus récente de la politique.\n",
    "\n",
    "* **Q-Learning**\n",
    "\n",
    "Les méthodes de cette famille apprennent un approximateur $Q_\\theta(s,a)$ de la fonction valeur d'action optimale, $Q^*(s, a)$. Elles utilisent généralement une fonction objective basée sur l'équation de Bellman. Cette optimisation est presque toujours effectuée **hors-politique**, chaque mise à jour peut utiliser les données collectées à tout moment pendant l'entrainement, quelle que soit la manière dont l'agent a choisi d'explorer l'environnement lorsque les données ont été obtenues. La politique correspondante est obtenue via la connexion entre $Q^*$ et $\\pi^*$: les actions prises par l'agent Q-learning sont données par\n",
    "\n",
    "$$ a(s)=\\arg \\underset{a}{\\max} Q_\\theta(s,a) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
